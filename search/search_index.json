{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Digital Developer Conference Kubernetes Lab \u00b6 In this hands-on lab, you will explore using the IBM Cloud Kubernetes Service using a classic Kubernetes example application. To make it interesting for experienced users, we've added a little twist to the Guestbook application. If you're new to Kubernetes, don't worry, we'll start with the fundamentals. And if you're experienced with Kubernetes, there's something that's probably new for you as well so hang in there as we all come up to speed. Lab Outline: Lab setup - get web terminal and content Deploy an application on Kubernetes Scale and Update Deployments Installing the Operator Framework and IBM Operator Creating an instance of Tone Analyzer Deploy the Guestbook Application with the Tone Analyzer Info In the exercises that follow you will see the actual command to run, followed by a separate example of running the command with the expected output. You only need to run the first example and never need to run a command you see preceded by a \"$\". You can even use the copy button on the right side of the command to make copying easier. Ready? Let's get started by setting up a web terminal and getting the code","title":"About the workshop"},{"location":"#welcome-to-the-digital-developer-conference-kubernetes-lab","text":"In this hands-on lab, you will explore using the IBM Cloud Kubernetes Service using a classic Kubernetes example application. To make it interesting for experienced users, we've added a little twist to the Guestbook application. If you're new to Kubernetes, don't worry, we'll start with the fundamentals. And if you're experienced with Kubernetes, there's something that's probably new for you as well so hang in there as we all come up to speed. Lab Outline: Lab setup - get web terminal and content Deploy an application on Kubernetes Scale and Update Deployments Installing the Operator Framework and IBM Operator Creating an instance of Tone Analyzer Deploy the Guestbook Application with the Tone Analyzer Info In the exercises that follow you will see the actual command to run, followed by a separate example of running the command with the expected output. You only need to run the first example and never need to run a command you see preceded by a \"$\". You can even use the copy button on the right side of the command to make copying easier. Ready? Let's get started by setting up a web terminal and getting the code","title":"Welcome to the Digital Developer Conference Kubernetes Lab"},{"location":"exercise-0/","text":"Lab setup - get web terminal and content \u00b6 You will already need an IBM Cloud account and an IBM Kubernetes Service cluster (free or paid) in order to proceed. You should have completed this in the previous module. Set up the web terminal \u00b6 Log in to the IBM Cloud to access the dashboard. Click on the top right terminal icon to launch the IBM Cloud Shell . Run the ibmcloud ks clusters command to verify the terminal and setup for access to the cluster ibmcloud ks clusters it's ok to ignore warnings you may see about versions of plugins or kubernetes cluster versions Configure the kubectl cli available within the terminal for access to your cluster. ibmcloud ks cluster config --cluster mycluster Verify access to the Kubernetes API. kubectl get namespace You should see output similar to the following, if so, then your're ready to continue. NAME STATUS AGE default Active 125m ibm-cert-store Active 121m ibm-system Active 124m kube-node-lease Active 125m kube-public Active 125m kube-system Active 125m Clone the lab repository \u00b6 In some of the labs, you will be creating Kubernetes resources by applyig .yaml files representing the desired state for the resource. To save typing in all of that, clone the lab repository into your web terminal: git clone https://github.com/timroster/digidevcon-iks you should see: Cloning into 'digidevcon-iks'... remote: Enumerating objects: 61, done. remote: Counting objects: 100% (61/61), done. remote: Compressing objects: 100% (44/44), done. remote: Total 61 (delta 13), reused 61 (delta 13), pack-reused 0 Unpacking objects: 100% (61/61), done. Continue on to the next exercise .","title":"Lab setup - get web terminal and content"},{"location":"exercise-0/#lab-setup-get-web-terminal-and-content","text":"You will already need an IBM Cloud account and an IBM Kubernetes Service cluster (free or paid) in order to proceed. You should have completed this in the previous module.","title":"Lab setup - get web terminal and content"},{"location":"exercise-0/#set-up-the-web-terminal","text":"Log in to the IBM Cloud to access the dashboard. Click on the top right terminal icon to launch the IBM Cloud Shell . Run the ibmcloud ks clusters command to verify the terminal and setup for access to the cluster ibmcloud ks clusters it's ok to ignore warnings you may see about versions of plugins or kubernetes cluster versions Configure the kubectl cli available within the terminal for access to your cluster. ibmcloud ks cluster config --cluster mycluster Verify access to the Kubernetes API. kubectl get namespace You should see output similar to the following, if so, then your're ready to continue. NAME STATUS AGE default Active 125m ibm-cert-store Active 121m ibm-system Active 124m kube-node-lease Active 125m kube-public Active 125m kube-system Active 125m","title":"Set up the web terminal"},{"location":"exercise-0/#clone-the-lab-repository","text":"In some of the labs, you will be creating Kubernetes resources by applyig .yaml files representing the desired state for the resource. To save typing in all of that, clone the lab repository into your web terminal: git clone https://github.com/timroster/digidevcon-iks you should see: Cloning into 'digidevcon-iks'... remote: Enumerating objects: 61, done. remote: Counting objects: 100% (61/61), done. remote: Compressing objects: 100% (44/44), done. remote: Total 61 (delta 13), reused 61 (delta 13), pack-reused 0 Unpacking objects: 100% (61/61), done. Continue on to the next exercise .","title":"Clone the lab repository"},{"location":"exercise-1/","text":"Deploy an application on Kubernetes \u00b6 In this part of the lab we will deploy an application called guestbook that has already been built and uploaded to DockerHub under the name ibmcom/guestbook:v1 . In Kubernetes, creating an application means deploying a set of pods that run containers. In this lab, you will begin with the most simple scenario of creating a deployment with a single pod using the kubectl cli. Create the application and service \u00b6 Create the guestbook application deployment: kubectl create deployment guestbook --image=ibmcom/guestbook:v1 This action will take a bit of time. To check the status of the running application, you can use: kubectl get pods You should see output similar to the following: $ kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-59bd679fdc-bxdg7 0/1 ContainerCreating 0 1m Eventually, the status should show up as Running . $ kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-59bd679fdc-bxdg7 1/1 Running 0 1m The end result of the run command is not just the pod containing our application containers, but a Deployment resource that manages the lifecycle of those pods. Once the status reads Running , we need to expose that deployment as a Service so that it can be accessed. By specifying a service type of NodePort , the service will also be mapped to a high-numbered port on each cluster node. The guestbook application listens on port 3000, so this is also specified in the command. Run: kubectl expose deployment guestbook --type=\"NodePort\" --port=3000 $ kubectl expose deployment guestbook --type = \"NodePort\" --port = 3000 service \"guestbook\" exposed To find the port used on that worker node, examine your new service: kubectl get service guestbook $ kubectl get service guestbook NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE guestbook NodePort 172.21.12.235 <none> 3000:30805/TCP 1m The output shows that the <nodeport> is 30805 . The service will take incoming connections to the high numbered port, 30805 and forward to port 3000 to the container inside the pod. For a service of type NodePort, a port in the range 30000-32767 is automatically chosen, and could be different for you. guestbook is now running on your cluster, and exposed to the internet. We need to find out where it is accessible. The worker nodes running in the container service get external IP addresses. Run $ ibmcloud cs workers <name-of-cluster> , and note the public IP listed on the <public-IP> line. ibmcloud ks workers -c mycluster $ ibmcloud ks workers -c mycluster OK ID Public IP Private IP Flavor State Status Zone Version kube-hou02-pa1e3ee39f549640aebea69a444f51fe55-w1 184.172.252.167 10.76.194.30 free normal Ready hou02 1.14.7_1535 We can see that our <public-IP> is 184.172.252.167 . Now that you have both the address and the port, you can now access the application in the web browser at <public-IP>:<nodeport> . In the example case this is 184.172.252.167:30805 . Enter in a browser tab your IP address and NodePort for your deployment. Try out the guestbook by putting in a few entries. Keep this browser tab handy as you can use it in the next exercise as well. Congratulations, you've now deployed an application to Kubernetes! Understanding what happened \u00b6 At its core, you can think of Kubernetes as being a highly-available database and a collection of watchers and controllers. Kubernetes objects and their required metadata, such as a name and their desired state, are stored in this database and the watchers and controllers act to ensure that the configuration of actual resources in the cluster matches the state stored in the database. Included in Kubernetes are a number of basic objects necessary for supporting applications as well as abstractions to simplify the configuration and management of applications. The most common basic object is a pod which encapsulates one or more containers along with storage resources, a unique network address and configuration options. The pod reflects the smallest unit of deployment. Although pods are technically transient, they will usually run until something destroys them, either a human operator or a controller. A Deployment is an abstraction that you can use to create a deployment of an application. It provides support for horizontally scaling pods, updating the container image used by the pods and also rollbacks. To create your application, you used the kubectl command to create a deployment object and provided a name for the deployment, \"guestbook\", and also the container image to use. These options were combined with defaults for the object to create the desired state that was stored in the database. Reconciliation of the desired state resulted in a single pod being started in the cluster. Then, you used the kubectl expose command to make the deployment resource accessible both inside and outside of the cluster. This command creates a Service for a number of different resource types (deployment, replica set, replication controler, pod) to allow access to a network port on the resource. You can use this deployment in the next lab of this course","title":"Deploy an application on Kubernetes"},{"location":"exercise-1/#deploy-an-application-on-kubernetes","text":"In this part of the lab we will deploy an application called guestbook that has already been built and uploaded to DockerHub under the name ibmcom/guestbook:v1 . In Kubernetes, creating an application means deploying a set of pods that run containers. In this lab, you will begin with the most simple scenario of creating a deployment with a single pod using the kubectl cli.","title":"Deploy an application on Kubernetes"},{"location":"exercise-1/#create-the-application-and-service","text":"Create the guestbook application deployment: kubectl create deployment guestbook --image=ibmcom/guestbook:v1 This action will take a bit of time. To check the status of the running application, you can use: kubectl get pods You should see output similar to the following: $ kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-59bd679fdc-bxdg7 0/1 ContainerCreating 0 1m Eventually, the status should show up as Running . $ kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-59bd679fdc-bxdg7 1/1 Running 0 1m The end result of the run command is not just the pod containing our application containers, but a Deployment resource that manages the lifecycle of those pods. Once the status reads Running , we need to expose that deployment as a Service so that it can be accessed. By specifying a service type of NodePort , the service will also be mapped to a high-numbered port on each cluster node. The guestbook application listens on port 3000, so this is also specified in the command. Run: kubectl expose deployment guestbook --type=\"NodePort\" --port=3000 $ kubectl expose deployment guestbook --type = \"NodePort\" --port = 3000 service \"guestbook\" exposed To find the port used on that worker node, examine your new service: kubectl get service guestbook $ kubectl get service guestbook NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE guestbook NodePort 172.21.12.235 <none> 3000:30805/TCP 1m The output shows that the <nodeport> is 30805 . The service will take incoming connections to the high numbered port, 30805 and forward to port 3000 to the container inside the pod. For a service of type NodePort, a port in the range 30000-32767 is automatically chosen, and could be different for you. guestbook is now running on your cluster, and exposed to the internet. We need to find out where it is accessible. The worker nodes running in the container service get external IP addresses. Run $ ibmcloud cs workers <name-of-cluster> , and note the public IP listed on the <public-IP> line. ibmcloud ks workers -c mycluster $ ibmcloud ks workers -c mycluster OK ID Public IP Private IP Flavor State Status Zone Version kube-hou02-pa1e3ee39f549640aebea69a444f51fe55-w1 184.172.252.167 10.76.194.30 free normal Ready hou02 1.14.7_1535 We can see that our <public-IP> is 184.172.252.167 . Now that you have both the address and the port, you can now access the application in the web browser at <public-IP>:<nodeport> . In the example case this is 184.172.252.167:30805 . Enter in a browser tab your IP address and NodePort for your deployment. Try out the guestbook by putting in a few entries. Keep this browser tab handy as you can use it in the next exercise as well. Congratulations, you've now deployed an application to Kubernetes!","title":"Create the application and service"},{"location":"exercise-1/#understanding-what-happened","text":"At its core, you can think of Kubernetes as being a highly-available database and a collection of watchers and controllers. Kubernetes objects and their required metadata, such as a name and their desired state, are stored in this database and the watchers and controllers act to ensure that the configuration of actual resources in the cluster matches the state stored in the database. Included in Kubernetes are a number of basic objects necessary for supporting applications as well as abstractions to simplify the configuration and management of applications. The most common basic object is a pod which encapsulates one or more containers along with storage resources, a unique network address and configuration options. The pod reflects the smallest unit of deployment. Although pods are technically transient, they will usually run until something destroys them, either a human operator or a controller. A Deployment is an abstraction that you can use to create a deployment of an application. It provides support for horizontally scaling pods, updating the container image used by the pods and also rollbacks. To create your application, you used the kubectl command to create a deployment object and provided a name for the deployment, \"guestbook\", and also the container image to use. These options were combined with defaults for the object to create the desired state that was stored in the database. Reconciliation of the desired state resulted in a single pod being started in the cluster. Then, you used the kubectl expose command to make the deployment resource accessible both inside and outside of the cluster. This command creates a Service for a number of different resource types (deployment, replica set, replication controler, pod) to allow access to a network port on the resource. You can use this deployment in the next lab of this course","title":"Understanding what happened"},{"location":"exercise-2/","text":"Scale and Update Deployments \u00b6 In this lab, you'll learn how to update the number of instances in a deployment has and how to roll out an update of your application on Kubernetes with zero downtime. A key requirement of a container orchestration system is to automate the management steps of applications. Scaling up/down and handling application updates with support for rollback are a couple of essential use cases (but not the only ones). For this lab, you need a running deployment of the guestbook application from the previous lab. Scale apps with replicas \u00b6 A replica is a copy of a pod that contains a running service. By having multiple replicas of a pod, you can ensure your deployment has the available resources to handle increasing load on your application. kubectl provides a scale subcommand to change the size of an existing deployment. Let's increase our capacity from a single running instance of guestbook up to 10 instances using: kubectl scale --replicas=10 deployment guestbook $ kubectl scale --replicas = 10 deployment guestbook deployment \"guestbook\" scaled If you remember back to the architecture overview in the previous exercise, the kubectl scale command updates the desired state in the etcd database in Kubernetes. The watchers and controllers will now try to make reality match the desired state of 10 replicas by starting 9 new pods with the same configuration as the first. To see your changes being rolled out, you can run: ```text kubectl rollout status deployment guestbook The rollout might occur so quickly that you may only see `deployment \"guestbook\" successfully rolled out` for the output. ```console $ kubectl rollout status deployment guestbook Waiting for rollout to finish: 1 of 10 updated replicas are available... Waiting for rollout to finish: 2 of 10 updated replicas are available... Waiting for rollout to finish: 3 of 10 updated replicas are available... Waiting for rollout to finish: 4 of 10 updated replicas are available... Waiting for rollout to finish: 5 of 10 updated replicas are available... Waiting for rollout to finish: 6 of 10 updated replicas are available... Waiting for rollout to finish: 7 of 10 updated replicas are available... Waiting for rollout to finish: 8 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... deployment \"guestbook\" successfully rolled out Once the rollout has finished, ensure your pods are running by using: ```text kubectl get pods You should see output listing 10 replicas of your deployment: ```console $ kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-562211614-1tqm7 1/1 Running 0 1d guestbook-562211614-1zqn4 1/1 Running 0 2m guestbook-562211614-5htdz 1/1 Running 0 2m guestbook-562211614-6h04h 1/1 Running 0 2m guestbook-562211614-ds9hb 1/1 Running 0 2m guestbook-562211614-nb5qp 1/1 Running 0 2m guestbook-562211614-vtfp2 1/1 Running 0 2m guestbook-562211614-vz5qw 1/1 Running 0 2m guestbook-562211614-zksw3 1/1 Running 0 2m guestbook-562211614-zsp0j 1/1 Running 0 2m Update and roll back apps \u00b6 Kubernetes allows you to do a rolling upgrade of your application to a new container image. Kubernetes allows you to easily update the running image but also allows you to easily undo a rollout if a problem is discovered during or after deployment. In the previous lab, we used an image with a v1 tag. For our upgrade, we'll use the image with the v2 tag. To update and roll back: Using kubectl , you can now update your deployment to use the v2 image. kubectl allows you to change details about existing resources with the set subcommand. We can use it to change the image being used. kubectl set image deployment/guestbook guestbook=ibmcom/guestbook:v2 Note that a pod could have multiple containers, each with its own name. Each image can be changed individually or all at once by referring to the name. In the case of our guestbook Deployment, the container name is also guestbook . Multiple containers can be updated at the same time. ( More information .) Check the status of the rollout. The rollout might occur so quickly that you may only see deployment \"guestbook\" successfully rolled out for the output. kubectl rollout status deployment/guestbook $ kubectl rollout status deployment/guestbook Waiting for rollout to finish: 2 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 9 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... deployment \"guestbook\" successfully rolled out Test the application as before, by accessing <public-IP>:<nodeport> (use the same as the previous lab) in the browser to confirm your new code is active. Remember, to get the \"nodeport\" and \"public-ip\" use: `text kubectl describe service guestbook and (replace mycluster below if you used a different name for your cluster): text ibmcloud ks workers -c mycluster To verify that you're running \"v2\" of guestbook, press down shift while clicking on reload to discard the cached content and look at the title of the page, it should now be Guestbook - v2 If you want to undo your latest rollout, use: kubectl rollout undo deployment guestbook You can then use kubectl rollout status deployment/guestbook to see the status. When doing a rollout, you see references to old replicas and new replicas. The old replicas are the original 10 pods deployed when we scaled the application. The new replicas come from the newly created pods with the different image. All of these pods are owned by the Deployment. The deployment manages these two sets of pods with a resource called a ReplicaSet. We can see the guestbook ReplicaSets with: kubectl get replicasets -l app=guestbook $ kubectl get replicasets -l app = guestbook NAME DESIRED CURRENT READY AGE guestbook-5f5548d4f 10 10 10 21m guestbook-768cc55c78 0 0 0 3h Before we continue, let's delete the application so we can learn about a different way to achieve the same results by using resource files instead of providing command line options. To remove the deployment, use: kubectl delete deployment guestbook To remove the service,use: kubectl delete service guestbook Deeper dive on configuring resources \u00b6 Although it was convenient to create the guestbook deployment using the cli, in practice most applications and other kubernetes objects are created using configuration files in .yaml format. For example, the guestbook-deployment.yaml file is an example configuration file that would deploy the guestbook image with a total of 3 instances. This file shows the key parts needed for each kubenetes object. After the API version and resource type, there is a metadata section which specfies the name of the resource and a set of labels. Then, there is a spec section which defines the desired state. First, there's the definition of the replica set object for the deployment. Within the replica set , there is a template for the pod controlled by the set. Within this template, you can find labels applied at the pod level and the spec for the container(s) that will be deployed with each pod. In the last section of this lab, you will again deploy the guestbook application, along with other objects for a multi-tier application using resource files. After these are deleted, proceed to the next lab of this course","title":"Scale and Update Deployments"},{"location":"exercise-2/#scale-and-update-deployments","text":"In this lab, you'll learn how to update the number of instances in a deployment has and how to roll out an update of your application on Kubernetes with zero downtime. A key requirement of a container orchestration system is to automate the management steps of applications. Scaling up/down and handling application updates with support for rollback are a couple of essential use cases (but not the only ones). For this lab, you need a running deployment of the guestbook application from the previous lab.","title":"Scale and Update Deployments"},{"location":"exercise-2/#scale-apps-with-replicas","text":"A replica is a copy of a pod that contains a running service. By having multiple replicas of a pod, you can ensure your deployment has the available resources to handle increasing load on your application. kubectl provides a scale subcommand to change the size of an existing deployment. Let's increase our capacity from a single running instance of guestbook up to 10 instances using: kubectl scale --replicas=10 deployment guestbook $ kubectl scale --replicas = 10 deployment guestbook deployment \"guestbook\" scaled If you remember back to the architecture overview in the previous exercise, the kubectl scale command updates the desired state in the etcd database in Kubernetes. The watchers and controllers will now try to make reality match the desired state of 10 replicas by starting 9 new pods with the same configuration as the first. To see your changes being rolled out, you can run: ```text kubectl rollout status deployment guestbook The rollout might occur so quickly that you may only see `deployment \"guestbook\" successfully rolled out` for the output. ```console $ kubectl rollout status deployment guestbook Waiting for rollout to finish: 1 of 10 updated replicas are available... Waiting for rollout to finish: 2 of 10 updated replicas are available... Waiting for rollout to finish: 3 of 10 updated replicas are available... Waiting for rollout to finish: 4 of 10 updated replicas are available... Waiting for rollout to finish: 5 of 10 updated replicas are available... Waiting for rollout to finish: 6 of 10 updated replicas are available... Waiting for rollout to finish: 7 of 10 updated replicas are available... Waiting for rollout to finish: 8 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... deployment \"guestbook\" successfully rolled out Once the rollout has finished, ensure your pods are running by using: ```text kubectl get pods You should see output listing 10 replicas of your deployment: ```console $ kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-562211614-1tqm7 1/1 Running 0 1d guestbook-562211614-1zqn4 1/1 Running 0 2m guestbook-562211614-5htdz 1/1 Running 0 2m guestbook-562211614-6h04h 1/1 Running 0 2m guestbook-562211614-ds9hb 1/1 Running 0 2m guestbook-562211614-nb5qp 1/1 Running 0 2m guestbook-562211614-vtfp2 1/1 Running 0 2m guestbook-562211614-vz5qw 1/1 Running 0 2m guestbook-562211614-zksw3 1/1 Running 0 2m guestbook-562211614-zsp0j 1/1 Running 0 2m","title":"Scale apps with replicas"},{"location":"exercise-2/#update-and-roll-back-apps","text":"Kubernetes allows you to do a rolling upgrade of your application to a new container image. Kubernetes allows you to easily update the running image but also allows you to easily undo a rollout if a problem is discovered during or after deployment. In the previous lab, we used an image with a v1 tag. For our upgrade, we'll use the image with the v2 tag. To update and roll back: Using kubectl , you can now update your deployment to use the v2 image. kubectl allows you to change details about existing resources with the set subcommand. We can use it to change the image being used. kubectl set image deployment/guestbook guestbook=ibmcom/guestbook:v2 Note that a pod could have multiple containers, each with its own name. Each image can be changed individually or all at once by referring to the name. In the case of our guestbook Deployment, the container name is also guestbook . Multiple containers can be updated at the same time. ( More information .) Check the status of the rollout. The rollout might occur so quickly that you may only see deployment \"guestbook\" successfully rolled out for the output. kubectl rollout status deployment/guestbook $ kubectl rollout status deployment/guestbook Waiting for rollout to finish: 2 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 9 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... deployment \"guestbook\" successfully rolled out Test the application as before, by accessing <public-IP>:<nodeport> (use the same as the previous lab) in the browser to confirm your new code is active. Remember, to get the \"nodeport\" and \"public-ip\" use: `text kubectl describe service guestbook and (replace mycluster below if you used a different name for your cluster): text ibmcloud ks workers -c mycluster To verify that you're running \"v2\" of guestbook, press down shift while clicking on reload to discard the cached content and look at the title of the page, it should now be Guestbook - v2 If you want to undo your latest rollout, use: kubectl rollout undo deployment guestbook You can then use kubectl rollout status deployment/guestbook to see the status. When doing a rollout, you see references to old replicas and new replicas. The old replicas are the original 10 pods deployed when we scaled the application. The new replicas come from the newly created pods with the different image. All of these pods are owned by the Deployment. The deployment manages these two sets of pods with a resource called a ReplicaSet. We can see the guestbook ReplicaSets with: kubectl get replicasets -l app=guestbook $ kubectl get replicasets -l app = guestbook NAME DESIRED CURRENT READY AGE guestbook-5f5548d4f 10 10 10 21m guestbook-768cc55c78 0 0 0 3h Before we continue, let's delete the application so we can learn about a different way to achieve the same results by using resource files instead of providing command line options. To remove the deployment, use: kubectl delete deployment guestbook To remove the service,use: kubectl delete service guestbook","title":"Update and roll back apps"},{"location":"exercise-2/#deeper-dive-on-configuring-resources","text":"Although it was convenient to create the guestbook deployment using the cli, in practice most applications and other kubernetes objects are created using configuration files in .yaml format. For example, the guestbook-deployment.yaml file is an example configuration file that would deploy the guestbook image with a total of 3 instances. This file shows the key parts needed for each kubenetes object. After the API version and resource type, there is a metadata section which specfies the name of the resource and a set of labels. Then, there is a spec section which defines the desired state. First, there's the definition of the replica set object for the deployment. Within the replica set , there is a template for the pod controlled by the set. Within this template, you can find labels applied at the pod level and the spec for the container(s) that will be deployed with each pod. In the last section of this lab, you will again deploy the guestbook application, along with other objects for a multi-tier application using resource files. After these are deleted, proceed to the next lab of this course","title":"Deeper dive on configuring resources"},{"location":"exercise-3/","text":"Installing the IBM Cloud Operator \u00b6 The Operator Framework provides support for Kubernetes-native extensions to manage custom resource types through operators. Many operators are available through operatorhub.io , including the IBM Cloud operator. The IBM Cloud operator simplifies the creation of IBM Cloud services and resouces and binding credentials from these resources into a Kubernetes cluster. The instructions in this guide are adapted from the IBM Developer tutorial Simplify the lifecycle management process for your Kubernetes apps with operators . With the IBM Cloud Kubernetes Service clusters at version 1.16 and later, the Operator Framework is already installed. So all you will need to do is install the IBM Cloud Operator. New clusters created after March 1 st , 2020 should all be at this level (or later). Installing the IBM Cloud operator \u00b6 With the OLM framework and marketplace support installed, it's time to install the IBM Cloud operator. This operator will use an IBM Cloud API key to manage resources within the cluster. Begin to configure the IBM Cloud operator by logging in to the IBM Cloud account using the IBM Cloud CLI. Start by logging in to IBM Cloud ibmcloud login Check your login default region to verify that there is a Cloud Foundry organization and space with this command: ibmcloud account orgs If there is output like: Getting orgs in region 'us-south' as myemail@example.com ... Retrieving current account... No organizations were found. Skip forward to the next step. Otherwise, if a Cloud Foundry organization is shown run the command: ibmcloud target --cf Verify that the ibmcloud CLI session is configured with a resource group for creation of the Tone Analyzer by the IBM Cloud operator. Run this command: ibmcloud target Check the output from the ibmcloud target command. If there is no resource group set, resulting in a message including: Resource group: No resource group targeted, use 'ibmcloud target -g RESOURCE_GROUP' then set the target resource group to the Default using: ibmcloud target -g Default some older IBM Cloud accounts may have a resource group named default , if you see an error using Default , repeat the command with default . The operator marketplace catalog provides a URL for the resources to install for each operator. Install the IBM Cloud Operator with the following command: curl -sL https://raw.githubusercontent.com/IBM/cloud-operators/master/hack/configure-operator.sh | bash -s -- install Check that the pod for the IBM Cloud operator is running with: kubectl get pods --namespace ibmcloud-operators You should see after a minute or two that the pod for the operator is running: $ kubectl get pods -n ibmcloud-operators NAME READY STATUS RESTARTS AGE ibmcloud-operator-76cb79d746-nksq8 1/1 Running 0 8m14s Understanding Operators \u00b6 The Operator Pattern is an emerging approach to extend through automation the expertise of human operators into the cluster environment. Operators are intended to support applications and management of other resources in and related to kubernetes clusters starting at installation, but continuing to day 2 operations of monitoring, backup, fault recovery and, of course, updates. Operators are custom code that uses the Kubernetes API (as a client) to implement a controller for a Custom Resource . Unlike the controllers built into the Kubernetes control plane which run on the Kubernetes master node, operators run outside of the Kubernetes control plan as pods on the worker nodes in the cluster. You can verify that fact by the kubectl get pods command above, which lists the pods of the operator running on a worker node. In addition to the IBM Cloud Operator, there are many operators that can manage resources within your cluster available from the Operator Hub . The Operator Hub includes many useful operators including operators that implement database installation, monitoring tools, application development frameworks, application runtimes and more. Your cluster now has the IBM Cloud operator installed. This operator is able to configure two custom resources in the cluster, a Service and a Binding . The Service defines a specific IBM Cloud service instance type to create, and the Binding specifies a named binding of a service instance to a secret in the cluster. For more details about the IBM Cloud operator see the project repository Continue by using the IBM Cloud operator to Create a Tone Analyzer service on IBM Cloud","title":"Installing the IBM Cloud Operator"},{"location":"exercise-3/#installing-the-ibm-cloud-operator","text":"The Operator Framework provides support for Kubernetes-native extensions to manage custom resource types through operators. Many operators are available through operatorhub.io , including the IBM Cloud operator. The IBM Cloud operator simplifies the creation of IBM Cloud services and resouces and binding credentials from these resources into a Kubernetes cluster. The instructions in this guide are adapted from the IBM Developer tutorial Simplify the lifecycle management process for your Kubernetes apps with operators . With the IBM Cloud Kubernetes Service clusters at version 1.16 and later, the Operator Framework is already installed. So all you will need to do is install the IBM Cloud Operator. New clusters created after March 1 st , 2020 should all be at this level (or later).","title":"Installing the IBM Cloud Operator"},{"location":"exercise-3/#installing-the-ibm-cloud-operator_1","text":"With the OLM framework and marketplace support installed, it's time to install the IBM Cloud operator. This operator will use an IBM Cloud API key to manage resources within the cluster. Begin to configure the IBM Cloud operator by logging in to the IBM Cloud account using the IBM Cloud CLI. Start by logging in to IBM Cloud ibmcloud login Check your login default region to verify that there is a Cloud Foundry organization and space with this command: ibmcloud account orgs If there is output like: Getting orgs in region 'us-south' as myemail@example.com ... Retrieving current account... No organizations were found. Skip forward to the next step. Otherwise, if a Cloud Foundry organization is shown run the command: ibmcloud target --cf Verify that the ibmcloud CLI session is configured with a resource group for creation of the Tone Analyzer by the IBM Cloud operator. Run this command: ibmcloud target Check the output from the ibmcloud target command. If there is no resource group set, resulting in a message including: Resource group: No resource group targeted, use 'ibmcloud target -g RESOURCE_GROUP' then set the target resource group to the Default using: ibmcloud target -g Default some older IBM Cloud accounts may have a resource group named default , if you see an error using Default , repeat the command with default . The operator marketplace catalog provides a URL for the resources to install for each operator. Install the IBM Cloud Operator with the following command: curl -sL https://raw.githubusercontent.com/IBM/cloud-operators/master/hack/configure-operator.sh | bash -s -- install Check that the pod for the IBM Cloud operator is running with: kubectl get pods --namespace ibmcloud-operators You should see after a minute or two that the pod for the operator is running: $ kubectl get pods -n ibmcloud-operators NAME READY STATUS RESTARTS AGE ibmcloud-operator-76cb79d746-nksq8 1/1 Running 0 8m14s","title":"Installing the IBM Cloud operator"},{"location":"exercise-3/#understanding-operators","text":"The Operator Pattern is an emerging approach to extend through automation the expertise of human operators into the cluster environment. Operators are intended to support applications and management of other resources in and related to kubernetes clusters starting at installation, but continuing to day 2 operations of monitoring, backup, fault recovery and, of course, updates. Operators are custom code that uses the Kubernetes API (as a client) to implement a controller for a Custom Resource . Unlike the controllers built into the Kubernetes control plane which run on the Kubernetes master node, operators run outside of the Kubernetes control plan as pods on the worker nodes in the cluster. You can verify that fact by the kubectl get pods command above, which lists the pods of the operator running on a worker node. In addition to the IBM Cloud Operator, there are many operators that can manage resources within your cluster available from the Operator Hub . The Operator Hub includes many useful operators including operators that implement database installation, monitoring tools, application development frameworks, application runtimes and more. Your cluster now has the IBM Cloud operator installed. This operator is able to configure two custom resources in the cluster, a Service and a Binding . The Service defines a specific IBM Cloud service instance type to create, and the Binding specifies a named binding of a service instance to a secret in the cluster. For more details about the IBM Cloud operator see the project repository Continue by using the IBM Cloud operator to Create a Tone Analyzer service on IBM Cloud","title":"Understanding Operators"},{"location":"exercise-4/","text":"Creating an instance of Tone Analyzer \u00b6 For an application running within a Kubernetes cluster to be able to access an IBM Cloud service, the service needs to be created and the credentials to access the service must be added to the cluster so that they can be read by deployed applications. The Kubernetes cluster running the application accessing the service instance can be anywhere, but in this case you'll be using your Kubernetes cluster on IBM Cloud. Create the service instance and bind to the cluster \u00b6 Change into the digidevcon-iks directory. apply the tone.yaml file. This file defines a Service and Binding resource: cd $HOME/digidevcon-iks Apply the tone.yaml file using kubectl. This file defines a Service and Binding resource: kubectl apply -f tone.yaml This file defines a Service and Binding resource and if successful there will be confirmation for both: $ kubectl apply -f tone.yaml service.ibmcloud.ibm.com/mytone created binding.ibmcloud.ibm.com/binding-tone created Check for the secret for the Tone Analyzer service instance added to the current namespace: kubectl get secret binding-tone You should see confirmation of the secret, but there may be a short delay as the credentials are obtained by the operator, so repeat this command until you no longer see an error like: Error from server (NotFound): secrets \"binding-tone\" not found $ kubectl get secret binding-tone NAME TYPE DATA AGE binding-tone Opaque 6 40s With the credentials added to the current namespace, you will be able to deploy guestbook application that uses the analyzer microservice. But first, let's do a little checking of the actions by the IBM Cloud operator. Check the IBM Cloud console - verify the Tone Analyzer serivce \u00b6 You can return to your IBM Cloud console and see that the tone analyzer service was created as specified in the tone.yaml resource file. Go back to your IBM Cloud tab in the browser and click on the words IBM Cloud on the upper left of the top menu. Now your Dashboard view will show a Services item under the Resource summary Click on the label Services in the Resource Summary , then click on the mytone label in the Services list. This will open up the control panel for the IBM Watson Tone Analyzer service. Click on the Show Credentials label to see your service API Key - make a note of it or just keep the credentials visible. Return to the Kubernetes Terminal tab in your web browser and enter this command to extract and decode the apikey from the secret created by the IBM Cloud Operator: kubectl get secret binding-tone -o=jsonpath='{.data.apikey}' | base64 -d && echo Notice how the string displayed is exactly the same as the service API Key visible from the control panel for the service. Lifecycle management with the IBM Cloud operator \u00b6 Let's take a look at the custom resource definition (CRD) file that was used in this exercise ( tone.yaml ). apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Service metadata: name: mytone spec: plan: lite serviceClass: tone-analyzer --- apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Binding metadata: name: binding-tone spec: serviceName: mytone role: Manager Note that the API version is different from what you may have seen in other resource files in this lab. Since Kubernetes objects are scoped by the API, there's no conflict with the re-use of the kind Service in this CRD. Recall that in the internal Kubernetes API, a resource of kind Service is used to expose network ports running on pods. Here, the Service object type is used to descibe an IBM Cloud platform service from the catalog. The operator uses the spec of the resource to select the desired IBM Cloud service type and offering plan. The role of the IBM Cloud operator is to manage instances of these services and also create a Binding to the service that is stored as a secret in the cluster. The operator will monitor the IBM Cloud account service instances. If something happens to the service instance, the operator will detect the change and take action. For example, if a the service instance is deleted, the operator will create a new service instance and update the credentials stored in the binding secret. Continue with deploying the guestbook application \u00b6 In the last section of the lab, you will use resource files and the kubectl create command to create a complex application topology that is very representative of a typical deployment on Kubernetes. This application will have a web front end written in go, it will persist data entered into the application in a redis database and the application will have an analyzer service witten in python that calls the IBM Watson Tone Analyzer service running outside of the Kubernetes cluster. Continue the exercise by deploying the guestbook application","title":"Creating an instance of Tone Analyzer"},{"location":"exercise-4/#creating-an-instance-of-tone-analyzer","text":"For an application running within a Kubernetes cluster to be able to access an IBM Cloud service, the service needs to be created and the credentials to access the service must be added to the cluster so that they can be read by deployed applications. The Kubernetes cluster running the application accessing the service instance can be anywhere, but in this case you'll be using your Kubernetes cluster on IBM Cloud.","title":"Creating an instance of Tone Analyzer"},{"location":"exercise-4/#create-the-service-instance-and-bind-to-the-cluster","text":"Change into the digidevcon-iks directory. apply the tone.yaml file. This file defines a Service and Binding resource: cd $HOME/digidevcon-iks Apply the tone.yaml file using kubectl. This file defines a Service and Binding resource: kubectl apply -f tone.yaml This file defines a Service and Binding resource and if successful there will be confirmation for both: $ kubectl apply -f tone.yaml service.ibmcloud.ibm.com/mytone created binding.ibmcloud.ibm.com/binding-tone created Check for the secret for the Tone Analyzer service instance added to the current namespace: kubectl get secret binding-tone You should see confirmation of the secret, but there may be a short delay as the credentials are obtained by the operator, so repeat this command until you no longer see an error like: Error from server (NotFound): secrets \"binding-tone\" not found $ kubectl get secret binding-tone NAME TYPE DATA AGE binding-tone Opaque 6 40s With the credentials added to the current namespace, you will be able to deploy guestbook application that uses the analyzer microservice. But first, let's do a little checking of the actions by the IBM Cloud operator.","title":"Create the service instance and bind to the cluster"},{"location":"exercise-4/#check-the-ibm-cloud-console-verify-the-tone-analyzer-serivce","text":"You can return to your IBM Cloud console and see that the tone analyzer service was created as specified in the tone.yaml resource file. Go back to your IBM Cloud tab in the browser and click on the words IBM Cloud on the upper left of the top menu. Now your Dashboard view will show a Services item under the Resource summary Click on the label Services in the Resource Summary , then click on the mytone label in the Services list. This will open up the control panel for the IBM Watson Tone Analyzer service. Click on the Show Credentials label to see your service API Key - make a note of it or just keep the credentials visible. Return to the Kubernetes Terminal tab in your web browser and enter this command to extract and decode the apikey from the secret created by the IBM Cloud Operator: kubectl get secret binding-tone -o=jsonpath='{.data.apikey}' | base64 -d && echo Notice how the string displayed is exactly the same as the service API Key visible from the control panel for the service.","title":"Check the IBM Cloud console - verify the Tone Analyzer serivce"},{"location":"exercise-4/#lifecycle-management-with-the-ibm-cloud-operator","text":"Let's take a look at the custom resource definition (CRD) file that was used in this exercise ( tone.yaml ). apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Service metadata: name: mytone spec: plan: lite serviceClass: tone-analyzer --- apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Binding metadata: name: binding-tone spec: serviceName: mytone role: Manager Note that the API version is different from what you may have seen in other resource files in this lab. Since Kubernetes objects are scoped by the API, there's no conflict with the re-use of the kind Service in this CRD. Recall that in the internal Kubernetes API, a resource of kind Service is used to expose network ports running on pods. Here, the Service object type is used to descibe an IBM Cloud platform service from the catalog. The operator uses the spec of the resource to select the desired IBM Cloud service type and offering plan. The role of the IBM Cloud operator is to manage instances of these services and also create a Binding to the service that is stored as a secret in the cluster. The operator will monitor the IBM Cloud account service instances. If something happens to the service instance, the operator will detect the change and take action. For example, if a the service instance is deleted, the operator will create a new service instance and update the credentials stored in the binding secret.","title":"Lifecycle management with the IBM Cloud operator"},{"location":"exercise-4/#continue-with-deploying-the-guestbook-application","text":"In the last section of the lab, you will use resource files and the kubectl create command to create a complex application topology that is very representative of a typical deployment on Kubernetes. This application will have a web front end written in go, it will persist data entered into the application in a redis database and the application will have an analyzer service witten in python that calls the IBM Watson Tone Analyzer service running outside of the Kubernetes cluster. Continue the exercise by deploying the guestbook application","title":"Continue with deploying the guestbook application"},{"location":"exercise-5/","text":"Deploy the Guestbook Application with the Tone Analyzer \u00b6 In this section, you will re-create the guestbook application, but this time with more components in a multi-tier architecture. This application uses the v2 version of the go application we used previously in the workshop as our web front end, and adds on 1) a Redis master for storage, 2) a replicated set of Redis slaves, and 3) a Python Flask application that calls the Watson Tone Analyzer service deployed in IBM Cloud. For all of these components, there are Kubernetes replication controllers, pods, and services. One of the main concerns with building a multi-tier application on Kubernetes, such as this one, is resolving dependencies between all of these seperately deployed components. In a multiple tier application, there are two primary ways that service dependencies can be resolved. The v2/guestbook/main.go code provides examples of each. For Redis, the master endpoint is discovered through environment variables. These environment variables are set when the Redis services are started, so the service resources need to be created before the guestbook replication controller starts the guestbook pods. For the analyzer service, an http request is made to a hostname, which allows for resource discovery at the time when the request is made. Consequently, we'll follow a specific order when creating the application components. First up, the Redis components will be created, then the guestbook application, and finally the analyzer microservice. Setup \u00b6 Continue by working in the web terminal. Change to the v2 folder where the deployment files reside: cd $HOME/digidevcon-iks/v2 Create the Redis master pod \u00b6 Use the redis-master-deployment.yaml file to create a replication controller and Redis master pod . The pod runs a Redis key-value server in a container. Using a replication controller is the preferred way to launch long-running pods, even for 1 replica, so that the pod inherits benefits from the self-healing mechanism in Kubernetes (i.e. keeps the pods alive). Use the redis-master-deployment.yaml file to create the Redis master deployment in your Kubernetes cluster: kubectl create -f redis-master-deployment.yaml $ kubectl create -f redis-master-deployment.yaml deployment.apps/redis-master created To verify that the redis-master controller is up, list the deployment and replicaset you created in the cluster with the kubectl get command (if you don't specify a --namespace , the current project/namespace will be used): kubectl get deploy this will show the current deployments in the namespace $ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE redis-master 1/1 1 1 15s kubectl get replicaset this will show the current deployments in the namespace $ kubectl get replicaset NAME DESIRED CURRENT READY AGE redis-master-7b7968db76 1 1 1 41s Result: The deployment creates the replicaset, which then creates the single Redis master pod. Verify that the redis-master pod is running, by listing the pods you created in cluster: kubectl get pods $ kubectl get pods NAME READY STATUS RESTARTS AGE redis-master-7b7968db76-8mjqg 1/1 Running 0 67s Result: You'll see a single Redis master pod (may take up to thirty seconds). Create the Redis master service \u00b6 A Kubernetes service is a named load balancer that proxies traffic to one or more pods. The services in a Kubernetes cluster are discoverable inside other pods via environment variables or DNS. Services find the pods to load balance based on pod labels. The pod that you created in previous step has the label app=redis and role=master . The selector field of the service determines which pods will receive the traffic sent to the service. Use the redis-master-service.yaml file to create the service in your Kubernetes cluster: kubectl create -f redis-master-service.yaml $ kubectl create -f redis-master-service.yaml service/redis-master created To verify that the redis-master service is up, list the services you created in the cluster: kubectl get services $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 172.21.0.1 <none> 443/TCP 8h redis-master ClusterIP 172.21.36.3 <none> 6379/TCP 10s ... Result: All new pods will see the redis-master service running on the host ( $REDIS_MASTER_SERVICE_HOST environment variable) at port 6379 , or running on redis-master:6379 . After the service is created, the service proxy on each node is configured to set up a proxy on the specified port (in our example, that's port 6379 ). Create the Redis slave pods \u00b6 The Redis master we created earlier is a single pod (REPLICAS = 1), while the Redis read slaves we are creating here are 'replicated' pods with 2 instances that will be started. In Kubernetes, a replication controller is responsible for managing the multiple instances of a replicated pod. Use the file redis-slave-deployment.yaml to create the replication controller: kubectl create -f redis-slave-deployment.yaml $ kubectl create -f redis-slave-deployment.yaml deployment.apps/redis-slave created To verify that the redis-slave controller is running: kubectl get rs $ kubectl get rs NAME DESIRED CURRENT READY AGE redis-master-7b7968db76 1 1 1 2m48s redis-slave-6944587c87 2 2 2 36s Result: The deployment creates the replicaset, which then creates configures the Redis slave pods through the redis-master service (name:port pair, in our example that's redis-master:6379 ). Verify that the Redis master and slaves pods are running: kubectl get pods $ kubectl get pods NAME READY STATUS RESTARTS AGE redis-master-7b7968db76-8mjqg 1/1 Running 0 3m25s redis-slave-6944587c87-4gvgj 1/1 Running 0 73s redis-slave-6944587c87-h66wp 1/1 Running 0 73s ... Result: You see the single Redis master and two Redis slave pods. Create the Redis slave service \u00b6 Just like the master, we want to have a service to proxy connections to the read slaves. In this case, in addition to discovery, the Redis slave service provides transparent load balancing to clients. Use the redis-slave-service.yaml file to create the Redis slave service: kubectl create -f redis-slave-service.yaml $ kubectl create -f redis-slave-service.yaml service/redis-slave created To verify that the redis-slave service is up, list the services you created: kubectl get services $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 172.21.0.1 <none> 443/TCP 8h redis-master ClusterIP 172.21.36.3 <none> 6379/TCP 2m32s redis-slave ClusterIP 172.21.251.35 <none> 6379/TCP 7s Result: The service is created and accessible at redis-slave:6379 by pods running in the project Create the guestbook pods \u00b6 This is a simple Go net/http ( negroni based) server that is configured to talk to either the slave or master services depending on whether the request is a read or a write. The pods we are creating expose a simple JSON interface and serves a jQuery-Ajax based UI. Like the Redis read slaves, these pods are also managed by a replication controller. Use the guestbook-deployment.yaml file to create the guestbook replication controller: kubectl create -f guestbook-deployment.yaml $ kubectl create -f guestbook-deployment.yaml deployment.apps/guestbook-v2 created Tip: If you want to modify the guestbook code it can be found in the guestbook directory, along with its Makefile. If you have pushed your custom image be sure to update the image property accordingly in the guestbook-deployment.yaml. Verify that the guestbook deployment is running: kubectl get deploy $ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE guestbook-v2 3/3 3 3 24s redis-master 1/1 1 1 5m37s redis-slave 2/2 2 2 3m58s Verify that the guestbook pods are running (it might take up to thirty seconds to create the pods): kubectl get pods $ kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v2-75fbf9c4d-c2m6s 1/1 Running 0 25s guestbook-v2-75fbf9c4d-nx2f2 1/1 Running 0 25s guestbook-v2-75fbf9c4d-p2x97 1/1 Running 0 25s redis-master-7b7968db76-8mjqg 1/1 Running 0 5m10s redis-slave-6944587c87-4gvgj 1/1 Running 0 2m58s redis-slave-6944587c87-h66wp 1/1 Running 0 2m58s Result: You see a single Redis master, two Redis slaves, and three guestbook pods. Create and expose the guestbook service \u00b6 Just like the others, we create a service to group the guestbook pods. Since guestbook uses a web application protocol we will expose it for access outside the cluster using a service of type `NodePort. Use the guestbook-service.yaml file to create the guestbook service: kubectl create -f guestbook-service.yaml Verify that the guestbook service is up by listing the services in the cluster: kubectl get services $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE guestbook NodePort 172.21.128.59 <none> 3000:30796/TCP 35s kubernetes ClusterIP 172.21.0.1 <none> 443/TCP 8h redis-master ClusterIP 172.21.36.3 <none> 6379/TCP 13m redis-slave ClusterIP 172.21.251.35 <none> 6379/TCP 11m Result: The service is created, and exposed as a NodePort and in this example is listening on 30796 . Create the analyzer pod \u00b6 This is a simple Python Flask application that creates a POST endpoint /tone and takes the input text and sends it to the Watson Tone Analyzer service. In the analyzer-deployment.yaml the spec for the pod defines environment variables for the service credentials by reading the secret binding-tone created by the IBM Cloud operator. Use the analyzer-deployment.yaml file to create the analyzer replication controller: kubectl create -f analyzer-deployment.yaml $ kubectl create -f analyzer-deployment.yaml deployment.apps/analyzer created Tip: If you want to modify the analyzer code it can be found in the analyzer directory, along with its Makefile. If you have pushed your custom image be sure to update the image property accordingly in the analyzer-deployment.yaml. Verify that the guestbook deployment is running: kubectl get deploy $ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE analyzer 0/1 1 0 16s guestbook-v2 3/3 3 3 14m redis-master 1/1 1 1 19m redis-slave 2/2 2 2 17m Create the analyzer service \u00b6 Create a service so that the guestbook application can call the analyzer pod Use the analyzer-service.yaml file to create the analyzer service: kubectl create -f analyzer-service.yaml To verify that the analyzer service is up, list the services created in the cluster: kubectl get services $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE analyzer ClusterIP 172.21.125.6 <none> 80/TCP 38s guestbook NodePort 172.21.128.59 <none> 3000:30796/TCP 5m54s kubernetes ClusterIP 172.21.0.1 <none> 443/TCP 8h redis-master ClusterIP 172.21.36.3 <none> 6379/TCP 19m redis-slave ClusterIP 172.21.251.35 <none> 6379/TCP 16m Result: The service is created View the guestbook \u00b6 You can now play with the guestbook that you just created by opening it in a browser, use the IP and NodePort for your deployment. Find the IP address for your cluster using this command (use the Public IP): ibmcloud ks workers -c mycluster $ ibmcloud ks workers -c mycluster ID Public IP Private IP Flavor State Status Zone Version kube-bmotc1dd0i2tk1jloing-mycluster-default-0000008d 184.172.252.167 10.76.195.211 free normal Ready hou02 1.14.7_1535 In this example the IP is: 184.172.252.167 Get the nodeport (it will be different from the first exercise): kubectl get service guestbook $ kubectl get service guestbook NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE guestbook NodePort 172.21.64.67 <none> 3000:30796/TCP 9m16s For this value of IP address and NodePort, you would use a url like http://184.172.252.167:30796 to access the guestbook. Result: The guestbook displays in your browser: Cleanup \u00b6 After you're done playing with the guestbook, you can cleanup by deleting the guestbook service and removing the associated resources that were created, including routes, forwarding rules, target pools, and Kubernetes replication controllers and services. Delete all the resources sourced by the files in the v2 directory: kubectl delete -f . $ kubectl delete -f . deployment.apps \"analyzer\" deleted service \"analyzer\" deleted deployment.apps \"guestbook-v2\" deleted service \"guestbook\" deleted deployment.apps \"redis-master\" deleted service \"redis-master\" deleted deployment.apps \"redis-slave\" deleted service \"redis-slave\" deleted","title":"Deploy the Guestbook Application with the Tone Analyzer"},{"location":"exercise-5/#deploy-the-guestbook-application-with-the-tone-analyzer","text":"In this section, you will re-create the guestbook application, but this time with more components in a multi-tier architecture. This application uses the v2 version of the go application we used previously in the workshop as our web front end, and adds on 1) a Redis master for storage, 2) a replicated set of Redis slaves, and 3) a Python Flask application that calls the Watson Tone Analyzer service deployed in IBM Cloud. For all of these components, there are Kubernetes replication controllers, pods, and services. One of the main concerns with building a multi-tier application on Kubernetes, such as this one, is resolving dependencies between all of these seperately deployed components. In a multiple tier application, there are two primary ways that service dependencies can be resolved. The v2/guestbook/main.go code provides examples of each. For Redis, the master endpoint is discovered through environment variables. These environment variables are set when the Redis services are started, so the service resources need to be created before the guestbook replication controller starts the guestbook pods. For the analyzer service, an http request is made to a hostname, which allows for resource discovery at the time when the request is made. Consequently, we'll follow a specific order when creating the application components. First up, the Redis components will be created, then the guestbook application, and finally the analyzer microservice.","title":"Deploy the Guestbook Application with the Tone Analyzer"},{"location":"exercise-5/#setup","text":"Continue by working in the web terminal. Change to the v2 folder where the deployment files reside: cd $HOME/digidevcon-iks/v2","title":"Setup"},{"location":"exercise-5/#create-the-redis-master-pod","text":"Use the redis-master-deployment.yaml file to create a replication controller and Redis master pod . The pod runs a Redis key-value server in a container. Using a replication controller is the preferred way to launch long-running pods, even for 1 replica, so that the pod inherits benefits from the self-healing mechanism in Kubernetes (i.e. keeps the pods alive). Use the redis-master-deployment.yaml file to create the Redis master deployment in your Kubernetes cluster: kubectl create -f redis-master-deployment.yaml $ kubectl create -f redis-master-deployment.yaml deployment.apps/redis-master created To verify that the redis-master controller is up, list the deployment and replicaset you created in the cluster with the kubectl get command (if you don't specify a --namespace , the current project/namespace will be used): kubectl get deploy this will show the current deployments in the namespace $ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE redis-master 1/1 1 1 15s kubectl get replicaset this will show the current deployments in the namespace $ kubectl get replicaset NAME DESIRED CURRENT READY AGE redis-master-7b7968db76 1 1 1 41s Result: The deployment creates the replicaset, which then creates the single Redis master pod. Verify that the redis-master pod is running, by listing the pods you created in cluster: kubectl get pods $ kubectl get pods NAME READY STATUS RESTARTS AGE redis-master-7b7968db76-8mjqg 1/1 Running 0 67s Result: You'll see a single Redis master pod (may take up to thirty seconds).","title":"Create the Redis master pod"},{"location":"exercise-5/#create-the-redis-master-service","text":"A Kubernetes service is a named load balancer that proxies traffic to one or more pods. The services in a Kubernetes cluster are discoverable inside other pods via environment variables or DNS. Services find the pods to load balance based on pod labels. The pod that you created in previous step has the label app=redis and role=master . The selector field of the service determines which pods will receive the traffic sent to the service. Use the redis-master-service.yaml file to create the service in your Kubernetes cluster: kubectl create -f redis-master-service.yaml $ kubectl create -f redis-master-service.yaml service/redis-master created To verify that the redis-master service is up, list the services you created in the cluster: kubectl get services $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 172.21.0.1 <none> 443/TCP 8h redis-master ClusterIP 172.21.36.3 <none> 6379/TCP 10s ... Result: All new pods will see the redis-master service running on the host ( $REDIS_MASTER_SERVICE_HOST environment variable) at port 6379 , or running on redis-master:6379 . After the service is created, the service proxy on each node is configured to set up a proxy on the specified port (in our example, that's port 6379 ).","title":"Create the Redis master service"},{"location":"exercise-5/#create-the-redis-slave-pods","text":"The Redis master we created earlier is a single pod (REPLICAS = 1), while the Redis read slaves we are creating here are 'replicated' pods with 2 instances that will be started. In Kubernetes, a replication controller is responsible for managing the multiple instances of a replicated pod. Use the file redis-slave-deployment.yaml to create the replication controller: kubectl create -f redis-slave-deployment.yaml $ kubectl create -f redis-slave-deployment.yaml deployment.apps/redis-slave created To verify that the redis-slave controller is running: kubectl get rs $ kubectl get rs NAME DESIRED CURRENT READY AGE redis-master-7b7968db76 1 1 1 2m48s redis-slave-6944587c87 2 2 2 36s Result: The deployment creates the replicaset, which then creates configures the Redis slave pods through the redis-master service (name:port pair, in our example that's redis-master:6379 ). Verify that the Redis master and slaves pods are running: kubectl get pods $ kubectl get pods NAME READY STATUS RESTARTS AGE redis-master-7b7968db76-8mjqg 1/1 Running 0 3m25s redis-slave-6944587c87-4gvgj 1/1 Running 0 73s redis-slave-6944587c87-h66wp 1/1 Running 0 73s ... Result: You see the single Redis master and two Redis slave pods.","title":"Create the Redis slave pods"},{"location":"exercise-5/#create-the-redis-slave-service","text":"Just like the master, we want to have a service to proxy connections to the read slaves. In this case, in addition to discovery, the Redis slave service provides transparent load balancing to clients. Use the redis-slave-service.yaml file to create the Redis slave service: kubectl create -f redis-slave-service.yaml $ kubectl create -f redis-slave-service.yaml service/redis-slave created To verify that the redis-slave service is up, list the services you created: kubectl get services $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 172.21.0.1 <none> 443/TCP 8h redis-master ClusterIP 172.21.36.3 <none> 6379/TCP 2m32s redis-slave ClusterIP 172.21.251.35 <none> 6379/TCP 7s Result: The service is created and accessible at redis-slave:6379 by pods running in the project","title":"Create the Redis slave service"},{"location":"exercise-5/#create-the-guestbook-pods","text":"This is a simple Go net/http ( negroni based) server that is configured to talk to either the slave or master services depending on whether the request is a read or a write. The pods we are creating expose a simple JSON interface and serves a jQuery-Ajax based UI. Like the Redis read slaves, these pods are also managed by a replication controller. Use the guestbook-deployment.yaml file to create the guestbook replication controller: kubectl create -f guestbook-deployment.yaml $ kubectl create -f guestbook-deployment.yaml deployment.apps/guestbook-v2 created Tip: If you want to modify the guestbook code it can be found in the guestbook directory, along with its Makefile. If you have pushed your custom image be sure to update the image property accordingly in the guestbook-deployment.yaml. Verify that the guestbook deployment is running: kubectl get deploy $ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE guestbook-v2 3/3 3 3 24s redis-master 1/1 1 1 5m37s redis-slave 2/2 2 2 3m58s Verify that the guestbook pods are running (it might take up to thirty seconds to create the pods): kubectl get pods $ kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v2-75fbf9c4d-c2m6s 1/1 Running 0 25s guestbook-v2-75fbf9c4d-nx2f2 1/1 Running 0 25s guestbook-v2-75fbf9c4d-p2x97 1/1 Running 0 25s redis-master-7b7968db76-8mjqg 1/1 Running 0 5m10s redis-slave-6944587c87-4gvgj 1/1 Running 0 2m58s redis-slave-6944587c87-h66wp 1/1 Running 0 2m58s Result: You see a single Redis master, two Redis slaves, and three guestbook pods.","title":"Create the guestbook pods"},{"location":"exercise-5/#create-and-expose-the-guestbook-service","text":"Just like the others, we create a service to group the guestbook pods. Since guestbook uses a web application protocol we will expose it for access outside the cluster using a service of type `NodePort. Use the guestbook-service.yaml file to create the guestbook service: kubectl create -f guestbook-service.yaml Verify that the guestbook service is up by listing the services in the cluster: kubectl get services $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE guestbook NodePort 172.21.128.59 <none> 3000:30796/TCP 35s kubernetes ClusterIP 172.21.0.1 <none> 443/TCP 8h redis-master ClusterIP 172.21.36.3 <none> 6379/TCP 13m redis-slave ClusterIP 172.21.251.35 <none> 6379/TCP 11m Result: The service is created, and exposed as a NodePort and in this example is listening on 30796 .","title":"Create and expose the guestbook service"},{"location":"exercise-5/#create-the-analyzer-pod","text":"This is a simple Python Flask application that creates a POST endpoint /tone and takes the input text and sends it to the Watson Tone Analyzer service. In the analyzer-deployment.yaml the spec for the pod defines environment variables for the service credentials by reading the secret binding-tone created by the IBM Cloud operator. Use the analyzer-deployment.yaml file to create the analyzer replication controller: kubectl create -f analyzer-deployment.yaml $ kubectl create -f analyzer-deployment.yaml deployment.apps/analyzer created Tip: If you want to modify the analyzer code it can be found in the analyzer directory, along with its Makefile. If you have pushed your custom image be sure to update the image property accordingly in the analyzer-deployment.yaml. Verify that the guestbook deployment is running: kubectl get deploy $ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE analyzer 0/1 1 0 16s guestbook-v2 3/3 3 3 14m redis-master 1/1 1 1 19m redis-slave 2/2 2 2 17m","title":"Create the analyzer pod"},{"location":"exercise-5/#create-the-analyzer-service","text":"Create a service so that the guestbook application can call the analyzer pod Use the analyzer-service.yaml file to create the analyzer service: kubectl create -f analyzer-service.yaml To verify that the analyzer service is up, list the services created in the cluster: kubectl get services $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE analyzer ClusterIP 172.21.125.6 <none> 80/TCP 38s guestbook NodePort 172.21.128.59 <none> 3000:30796/TCP 5m54s kubernetes ClusterIP 172.21.0.1 <none> 443/TCP 8h redis-master ClusterIP 172.21.36.3 <none> 6379/TCP 19m redis-slave ClusterIP 172.21.251.35 <none> 6379/TCP 16m Result: The service is created","title":"Create the analyzer service"},{"location":"exercise-5/#view-the-guestbook","text":"You can now play with the guestbook that you just created by opening it in a browser, use the IP and NodePort for your deployment. Find the IP address for your cluster using this command (use the Public IP): ibmcloud ks workers -c mycluster $ ibmcloud ks workers -c mycluster ID Public IP Private IP Flavor State Status Zone Version kube-bmotc1dd0i2tk1jloing-mycluster-default-0000008d 184.172.252.167 10.76.195.211 free normal Ready hou02 1.14.7_1535 In this example the IP is: 184.172.252.167 Get the nodeport (it will be different from the first exercise): kubectl get service guestbook $ kubectl get service guestbook NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE guestbook NodePort 172.21.64.67 <none> 3000:30796/TCP 9m16s For this value of IP address and NodePort, you would use a url like http://184.172.252.167:30796 to access the guestbook. Result: The guestbook displays in your browser:","title":"View the guestbook"},{"location":"exercise-5/#cleanup","text":"After you're done playing with the guestbook, you can cleanup by deleting the guestbook service and removing the associated resources that were created, including routes, forwarding rules, target pools, and Kubernetes replication controllers and services. Delete all the resources sourced by the files in the v2 directory: kubectl delete -f . $ kubectl delete -f . deployment.apps \"analyzer\" deleted service \"analyzer\" deleted deployment.apps \"guestbook-v2\" deleted service \"guestbook\" deleted deployment.apps \"redis-master\" deleted service \"redis-master\" deleted deployment.apps \"redis-slave\" deleted service \"redis-slave\" deleted","title":"Cleanup"}]}